{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exploring Naive Bayes Classifiers using [SMS Spam Collection Set from UCI](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection#)\n",
    "\n",
    "#### Analysis follows Chapter 4 of *Machine Learning with R* by Brett Lantz (though of course here we use Python, not R)\n",
    "\n",
    "#### Objective:  Use a classifier to predict whether an SMS message is spam or not, using accuracy (% correct) as the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import itertools\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib\n",
    "sns.set(style=\"white\", color_codes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data loading and exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data from https://archive.ics.uci.edu/ml/datasets/sms+spam+collection#\n",
    "data = pd.read_csv('SMSSpamCollection', sep='\\t', header=0, names=['Type','Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Type                                               Text\n",
       "0   ham                      Ok lar... Joking wif u oni...\n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "2   ham  U dun say so early hor... U c already then say...\n",
       "3   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "4  spam  FreeMsg Hey there darling it's been 3 week's n..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5571, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4824\n",
       "spam     747\n",
       "Name: Type, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying 3 different stemmers available in nltk package: Snowball, Porter, and Lancaster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Text</th>\n",
       "      <th>Stemmed</th>\n",
       "      <th>Porter</th>\n",
       "      <th>Lancaster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar... joke wif u oni...</td>\n",
       "      <td>Ok lar... joke wif u oni...</td>\n",
       "      <td>ok lar... jok wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entri in 2 a wkli comp to win fa cup fina...</td>\n",
       "      <td>free entri in 2 a wkli comp to win FA cup fina...</td>\n",
       "      <td>fre entry in 2 a wkly comp to win fa cup fin t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say so earli hor... u c alreadi then say...</td>\n",
       "      <td>U dun say so earli hor... U c alreadi then say...</td>\n",
       "      <td>u dun say so ear hor... u c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i don't think he goe to usf, he live aroun...</td>\n",
       "      <td>nah I don't think he goe to usf, he live aroun...</td>\n",
       "      <td>nah i don't think he goe to usf, he liv around...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>freemsg hey there darl it been 3 week now and ...</td>\n",
       "      <td>freemsg hey there darl it' been 3 week' now an...</td>\n",
       "      <td>freemsg hey ther darl it's been 3 week's now a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Type                                               Text  \\\n",
       "0   ham                      Ok lar... Joking wif u oni...   \n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2   ham  U dun say so early hor... U c already then say...   \n",
       "3   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "4  spam  FreeMsg Hey there darling it's been 3 week's n...   \n",
       "\n",
       "                                             Stemmed  \\\n",
       "0                        ok lar... joke wif u oni...   \n",
       "1  free entri in 2 a wkli comp to win fa cup fina...   \n",
       "2  u dun say so earli hor... u c alreadi then say...   \n",
       "3  nah i don't think he goe to usf, he live aroun...   \n",
       "4  freemsg hey there darl it been 3 week now and ...   \n",
       "\n",
       "                                              Porter  \\\n",
       "0                        Ok lar... joke wif u oni...   \n",
       "1  free entri in 2 a wkli comp to win FA cup fina...   \n",
       "2  U dun say so earli hor... U c alreadi then say...   \n",
       "3  nah I don't think he goe to usf, he live aroun...   \n",
       "4  freemsg hey there darl it' been 3 week' now an...   \n",
       "\n",
       "                                           Lancaster  \n",
       "0                         ok lar... jok wif u oni...  \n",
       "1  fre entry in 2 a wkly comp to win fa cup fin t...  \n",
       "2    u dun say so ear hor... u c already then say...  \n",
       "3  nah i don't think he goe to usf, he liv around...  \n",
       "4  freemsg hey ther darl it's been 3 week's now a...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stem_data(data, stemmer):\n",
    "    return data.apply(lambda x: ' '.join([stemmer.stem(y) for y in x.split(' ')]))\n",
    "\n",
    "data['Stemmed'] = stem_data(data['Text'], SnowballStemmer('english') )\n",
    "data['Porter'] = stem_data(data['Text'], PorterStemmer() )\n",
    "data['Lancaster'] = stem_data(data['Text'], LancasterStemmer() )\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code the 'Type' field as 0 or 1 for the classifier, and drop from the main dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = data['Type'].map({'ham':0, 'spam':1})\n",
    "X = data.drop(labels=['Type'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Stemmed</th>\n",
       "      <th>Porter</th>\n",
       "      <th>Lancaster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar... joke wif u oni...</td>\n",
       "      <td>Ok lar... joke wif u oni...</td>\n",
       "      <td>ok lar... jok wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entri in 2 a wkli comp to win fa cup fina...</td>\n",
       "      <td>free entri in 2 a wkli comp to win FA cup fina...</td>\n",
       "      <td>fre entry in 2 a wkly comp to win fa cup fin t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say so earli hor... u c alreadi then say...</td>\n",
       "      <td>U dun say so earli hor... U c alreadi then say...</td>\n",
       "      <td>u dun say so ear hor... u c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i don't think he goe to usf, he live aroun...</td>\n",
       "      <td>nah I don't think he goe to usf, he live aroun...</td>\n",
       "      <td>nah i don't think he goe to usf, he liv around...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>freemsg hey there darl it been 3 week now and ...</td>\n",
       "      <td>freemsg hey there darl it' been 3 week' now an...</td>\n",
       "      <td>freemsg hey ther darl it's been 3 week's now a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0                      Ok lar... Joking wif u oni...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2  U dun say so early hor... U c already then say...   \n",
       "3  Nah I don't think he goes to usf, he lives aro...   \n",
       "4  FreeMsg Hey there darling it's been 3 week's n...   \n",
       "\n",
       "                                             Stemmed  \\\n",
       "0                        ok lar... joke wif u oni...   \n",
       "1  free entri in 2 a wkli comp to win fa cup fina...   \n",
       "2  u dun say so earli hor... u c alreadi then say...   \n",
       "3  nah i don't think he goe to usf, he live aroun...   \n",
       "4  freemsg hey there darl it been 3 week now and ...   \n",
       "\n",
       "                                              Porter  \\\n",
       "0                        Ok lar... joke wif u oni...   \n",
       "1  free entri in 2 a wkli comp to win FA cup fina...   \n",
       "2  U dun say so earli hor... U c alreadi then say...   \n",
       "3  nah I don't think he goe to usf, he live aroun...   \n",
       "4  freemsg hey there darl it' been 3 week' now an...   \n",
       "\n",
       "                                           Lancaster  \n",
       "0                         ok lar... jok wif u oni...  \n",
       "1  fre entry in 2 a wkly comp to win fa cup fin t...  \n",
       "2    u dun say so ear hor... u c already then say...  \n",
       "3  nah i don't think he goe to usf, he liv around...  \n",
       "4  freemsg hey ther darl it's been 3 week's now a...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create test and training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create initial model on training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd like to investigate the effects of the following:\n",
    "\n",
    "1. Using a stemmer vs. no stemmer\n",
    "2. Using word-based vs. character-based n-grams\n",
    "2. Using tf-idf representation or raw n-gram counts\n",
    "3. Using different types of classifiers:  Naive Bayes, SGD, SVM\n",
    "\n",
    "And all of these will be cross-validated using different ranges of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GridSearchCV_results( vectorizer_list, classifier_list, field_list, X, y, score ):\n",
    "    '''\n",
    "    Runs the GridSearchCV function on given inputs:\n",
    "        Lists of:\n",
    "        - classifiers\n",
    "        - vectorizers\n",
    "        - input fields to classify\n",
    "        X, y are pandas dataframes\n",
    "        score is a string indicating the value to optimize, e.g. 'accuracy'\n",
    "    Output: dictionary where key = tuple of parameters, value = score\n",
    "    '''\n",
    "    results = {}\n",
    "\n",
    "    for v in vectorizer_list:\n",
    "        for c in classifier_list:\n",
    "        \n",
    "            pipeline = Pipeline([ (v[0], v[1]), (c[0], c[1]) ])\n",
    "\n",
    "            parameters = {}\n",
    "            parameters.update(v[2])\n",
    "            parameters.update(c[2])\n",
    "\n",
    "            grid_search = GridSearchCV(pipeline, parameters, scoring=score, verbose=0, n_jobs=4)\n",
    "            \n",
    "            for field in field_list:\n",
    "                results[(field, v[0], c[0])] = grid_search.fit(X[field],y)\n",
    "                \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train parameters first on just the original Text\n",
    "X_train_list = ['Text']\n",
    "\n",
    "# test effects of different vectorizers and their parameters\n",
    "vectorizer_list = [\n",
    "    \n",
    "    ('cv-word', CountVectorizer(analyzer='word'), {\n",
    "\n",
    "        'cv-word__max_features': (None, 2000, 5000, 10000),\n",
    "        'cv-word__ngram_range': ((1, 1), (1, 2)) })\n",
    "    \n",
    "    ,('cv-char', CountVectorizer(analyzer='char'), {\n",
    "\n",
    "        'cv-char__max_features': (None, 2000),\n",
    "        'cv-char__ngram_range': ((3, 3), (3, 4) )})\n",
    "    \n",
    "    ,('tfidf-word', TfidfVectorizer(analyzer='word'), {\n",
    "\n",
    "        'tfidf-word__max_features': (None, 2000, 5000, 10000),\n",
    "        'tfidf-word__ngram_range': ((1, 1), (1, 2)),\n",
    "        'tfidf-word__smooth_idf': (True, False),\n",
    "        'tfidf-word__norm': ('l2','l1', None)})\n",
    "    \n",
    "    ,('tfidf-char', TfidfVectorizer(analyzer='char'), {\n",
    "\n",
    "        'tfidf-char__max_features': (None, 2000),\n",
    "        'tfidf-char__ngram_range': ((3, 3), (3, 4)),\n",
    "        'tfidf-char__smooth_idf': (True, False),\n",
    "        'tfidf-char__norm': ('l2','l1', None)})\n",
    "]\n",
    "\n",
    "# test effects of different classifiers and their parameters\n",
    "classifier_list = [\n",
    "    \n",
    "    # naive bayes\n",
    "    ('mnb', MultinomialNB(), {'mnb__alpha': (1, .1, .01, .001, .0001, .00001)})\n",
    "    \n",
    "    # linear regression classifier\n",
    "    ,('sgd', SGDClassifier(loss='log'), {\n",
    "    'sgd__alpha': (0.01, 0.001, 0.0001, 0.00001, 0.000001),\n",
    "    'sgd__penalty': ('none', 'l1', 'l2', 'elasticnet'),\n",
    "    'sgd__l1_ratio': (.2, .5, .8)})\n",
    "    \n",
    "    # support vector machine\n",
    "    ,('svm-linear', SVC(kernel='linear'), {'svm-linear__C': (1.0, 10.0, 100.0)})\n",
    "    ,('svm-rbf', SVC(kernel='rbf'), {'svm-rbf__C': (1.0, 10.0, 100.0), 'svm-rbf__gamma': (1e-06, 1e-5, 1e-4)})\n",
    "    ,('svm-poly', SVC(kernel='poly'), {'svm-poly__C': (1.0, 10.0, 100.0),\n",
    "                                       'svm-poly__gamma': (1e-06, 1e-5, 1e-4), 'svm-poly__degree': (2, 3, 4)})\n",
    "]\n",
    "\n",
    "#run grid search on above parameters\n",
    "grid_results = GridSearchCV_results( \n",
    "    vectorizer_list, classifier_list, X_train_list, X_train, y_train, score = 'accuracy' \n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Text', 'cv-char', 'mnb'): 0.98999743524\n",
      "('Text', 'cv-char', 'sgd'): 0.986919723006\n",
      "('Text', 'cv-char', 'svm-linear'): 0.986150294947\n",
      "('Text', 'cv-char', 'svm-poly'): 0.873044370351\n",
      "('Text', 'cv-char', 'svm-rbf'): 0.986150294947\n",
      "('Text', 'cv-word', 'mnb'): 0.987432675045\n",
      "('Text', 'cv-word', 'sgd'): 0.986406770967\n",
      "('Text', 'cv-word', 'svm-linear'): 0.984098486791\n",
      "('Text', 'cv-word', 'svm-poly'): 0.872018466273\n",
      "('Text', 'cv-word', 'svm-rbf'): 0.976147730187\n",
      "('Text', 'tfidf-char', 'mnb'): 0.990510387279\n",
      "('Text', 'tfidf-char', 'sgd'): 0.990253911259\n",
      "('Text', 'tfidf-char', 'svm-linear'): 0.989228007181\n",
      "('Text', 'tfidf-char', 'svm-poly'): 0.971787637856\n",
      "('Text', 'tfidf-char', 'svm-rbf'): 0.987176199025\n",
      "('Text', 'tfidf-word', 'mnb'): 0.988202103103\n",
      "('Text', 'tfidf-word', 'sgd'): 0.988458579123\n",
      "('Text', 'tfidf-word', 'svm-linear'): 0.98974095922\n",
      "('Text', 'tfidf-word', 'svm-poly'): 0.931777378815\n",
      "('Text', 'tfidf-word', 'svm-rbf'): 0.982816106694\n"
     ]
    }
   ],
   "source": [
    "for key, val in sorted(grid_results.items()): \n",
    "    print(str(key) + ': ' + str(val.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best accuracy (>99%) is achieved by both Naive Bayes and linear regression classifiers, using tfidf at the character level.  The worst is the SVM with the polynomial kernel.\n",
    "\n",
    "Next steps:  try optimizing both Naive Bayes and linear regression to get even better accuracy.  But first, let's look at the SVM with the polynomial kernel just to see if it can be improved with parameter tuning, since it's so much less accurate than the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Additional parameter tuning for each algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM with the polynomial kernel has the worst results.  Before getting rid of it completely, let's try higher values of both C and gamma to see if that improves things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train parameters first on just the original Text\n",
    "X_train_list = ['Text']\n",
    "\n",
    "# test effects of different vectorizers and their parameters\n",
    "vectorizer_list = [\n",
    "    \n",
    "    ('cv-word', CountVectorizer(analyzer='word'), {\n",
    "\n",
    "        'cv-word__max_features': (None, 2000, 5000, 10000),\n",
    "        'cv-word__ngram_range': ((1, 1), (1, 2)) })\n",
    "    \n",
    "    ,('cv-char', CountVectorizer(analyzer='char'), {\n",
    "\n",
    "        'cv-char__max_features': (None, 2000),\n",
    "        'cv-char__ngram_range': ((3, 3), (3, 4) )})\n",
    "    \n",
    "    ,('tfidf-word', TfidfVectorizer(analyzer='word'), {\n",
    "\n",
    "        'tfidf-word__max_features': (None, 2000, 5000, 10000),\n",
    "        'tfidf-word__ngram_range': ((1, 1), (1, 2)),\n",
    "        'tfidf-word__smooth_idf': (True, False),\n",
    "        'tfidf-word__norm': ('l2','l1', None)})\n",
    "    \n",
    "    ,('tfidf-char', TfidfVectorizer(analyzer='char'), {\n",
    "\n",
    "        'tfidf-char__max_features': (None, 2000),\n",
    "        'tfidf-char__ngram_range': ((3, 3), (3, 4)),\n",
    "        'tfidf-char__smooth_idf': (True, False),\n",
    "        'tfidf-char__norm': ('l2','l1', None)})\n",
    "]\n",
    "\n",
    "# test effects of different classifiers and their parameters\n",
    "classifier_list = [\n",
    "    \n",
    "    # support vector machine\n",
    "    ('svm-poly', SVC(kernel='poly'), {'svm-poly__C': (500.0, 1000.0, 5000.0),\n",
    "                                       'svm-poly__gamma': (1e-03, 1e-2, 1e-1), 'svm-poly__degree': (2, 3, 4)})\n",
    "]\n",
    "\n",
    "#run grid search on above parameters\n",
    "svm_poly_results = GridSearchCV_results( \n",
    "    vectorizer_list, classifier_list, X_train_list, X_train, y_train, score = 'accuracy' \n",
    "    )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv-char Text: 0.975121826109\n",
      "{'svm-poly__degree': 2, 'svm-poly__gamma': 0.001, 'svm-poly__C': 5000.0, 'cv-char__max_features': 2000, 'cv-char__ngram_range': (3, 4)}\n",
      "cv-word Text: 0.96947935368\n",
      "{'cv-word__max_features': 2000, 'svm-poly__degree': 2, 'cv-word__ngram_range': (1, 1), 'svm-poly__gamma': 0.01, 'svm-poly__C': 1000.0}\n",
      "tfidf-char Text: 0.977943062324\n",
      "{'tfidf-char__norm': 'l2', 'tfidf-char__smooth_idf': True, 'svm-poly__degree': 2, 'tfidf-char__max_features': 2000, 'svm-poly__gamma': 0.1, 'svm-poly__C': 500.0, 'tfidf-char__ngram_range': (3, 4)}\n",
      "tfidf-word Text: 0.972557065914\n",
      "{'tfidf-word__smooth_idf': True, 'svm-poly__degree': 2, 'tfidf-word__max_features': 2000, 'tfidf-word__norm': 'l2', 'svm-poly__gamma': 0.1, 'svm-poly__C': 1000.0, 'tfidf-word__ngram_range': (1, 1)}\n"
     ]
    }
   ],
   "source": [
    "for key, val in sorted(svm_poly_results.items()): \n",
    "    print(key[1] + ' ' + key[0] + ': ' + str(val.best_score_))\n",
    "    print(val.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better, but still not as good as the other algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's look at SGD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv-char: 0.986919723006\n",
      "{'sgd__l1_ratio': 0.2, 'sgd__penalty': 'elasticnet', 'cv-char__max_features': None, 'sgd__alpha': 0.001, 'cv-char__ngram_range': (3, 3)}\n",
      "cv-word: 0.986406770967\n",
      "{'cv-word__max_features': 2000, 'sgd__l1_ratio': 0.2, 'cv-word__ngram_range': (1, 1), 'sgd__penalty': 'l2', 'sgd__alpha': 0.0001}\n",
      "tfidf-char: 0.990253911259\n",
      "{'sgd__l1_ratio': 0.5, 'tfidf-char__norm': 'l2', 'tfidf-char__smooth_idf': False, 'tfidf-char__ngram_range': (3, 3), 'sgd__alpha': 1e-06, 'tfidf-char__max_features': None, 'sgd__penalty': 'elasticnet'}\n",
      "tfidf-word: 0.988458579123\n",
      "{'sgd__l1_ratio': 0.2, 'tfidf-word__ngram_range': (1, 2), 'sgd__alpha': 1e-06, 'tfidf-word__norm': 'l2', 'tfidf-word__smooth_idf': False, 'sgd__penalty': 'elasticnet', 'tfidf-word__max_features': None}\n"
     ]
    }
   ],
   "source": [
    "for key, val in sorted(grid_results.items()): \n",
    "    if key[2] == 'sgd' :\n",
    "        print(key[1] + ': ' + str(val.best_score_))\n",
    "        print(val.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's stick to L2 and try a wider range of alphas, and also fix a few other parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-08\n",
      "3.59e-08\n",
      "1.29e-07\n",
      "4.64e-07\n",
      "1.67e-06\n",
      "5.99e-06\n",
      "2.15e-05\n",
      "7.74e-05\n",
      "0.000278\n",
      "0.001\n"
     ]
    }
   ],
   "source": [
    "for i in np.logspace(-8,-3, num=10):print('%.3g' % i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train parameters first on just the original Text\n",
    "X_train_list = ['Text']\n",
    "\n",
    "# test effects of different vectorizers and their parameters\n",
    "vectorizer_list = [\n",
    "    \n",
    "    ('cv-word', CountVectorizer(analyzer='word'), {\n",
    "\n",
    "        'cv-word__max_features': (None, 2000, 5000, 10000),\n",
    "        'cv-word__ngram_range': ((1, 1), (1, 2)) })\n",
    "    \n",
    "    ,('cv-char', CountVectorizer(analyzer='char'), {\n",
    "\n",
    "        'cv-char__max_features': (None, 2000),\n",
    "        'cv-char__ngram_range': ((3, 3), (3, 4) )})\n",
    "    \n",
    "    ,('tfidf-word', TfidfVectorizer(analyzer='word'), {\n",
    "\n",
    "        'tfidf-word__max_features': (None, 2000, 5000, 10000),\n",
    "        'tfidf-word__ngram_range': ((1, 1), (1, 2)),\n",
    "        'tfidf-word__smooth_idf': (True, False),\n",
    "        'tfidf-word__norm': ('l2','l1', None)})\n",
    "    \n",
    "    ,('tfidf-char', TfidfVectorizer(analyzer='char'), {\n",
    "\n",
    "        'tfidf-char__max_features': (None, 2000),\n",
    "        'tfidf-char__ngram_range': ((3, 3), (3, 4)),\n",
    "        'tfidf-char__smooth_idf': (True, False),\n",
    "        'tfidf-char__norm': ('l2','l1', None)})\n",
    "]\n",
    "\n",
    "# test effects of different classifiers and their parameters\n",
    "classifier_list = [\n",
    "\n",
    "    \n",
    "    # linear classifier\n",
    "    ('sgd', SGDClassifier(loss='log', penalty='l2'), {\n",
    "    'sgd__alpha': (1e-08,3.59e-08,1.29e-07,4.64e-07,1.67e-06,5.99e-06,2.15e-05,7.74e-05,0.000278,0.001)})\n",
    "    \n",
    "]\n",
    "\n",
    "#run grid search on above parameters\n",
    "sgd_results = GridSearchCV_results( \n",
    "    vectorizer_list, classifier_list, X_train_list, X_train, y_train, score = 'accuracy' \n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv-char: 0.986663246986\n",
      "{'cv-char__max_features': None, 'sgd__alpha': 0.001, 'cv-char__ngram_range': (3, 4)}\n",
      "cv-word: 0.985637342908\n",
      "{'cv-word__max_features': None, 'cv-word__ngram_range': (1, 1), 'sgd__alpha': 0.000278}\n",
      "tfidf-char: 0.988458579123\n",
      "{'tfidf-char__max_features': None, 'tfidf-char__norm': 'l2', 'tfidf-char__smooth_idf': True, 'tfidf-char__ngram_range': (3, 4), 'sgd__alpha': 1.67e-06}\n",
      "tfidf-word: 0.987176199025\n",
      "{'tfidf-word__norm': 'l2', 'tfidf-word__smooth_idf': True, 'tfidf-word__max_features': None, 'sgd__alpha': 4.64e-07, 'tfidf-word__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "for key, val in sorted(sgd_results.items()): \n",
    "    print(key[1] + ': ' + str(val.best_score_))\n",
    "    print(val.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still just under 99% for each of these.  \n",
    "\n",
    "MNB still seems to produce the best results so let's focus on that.  Let's fix the best vectorizer now and try optimizing:\n",
    "- alpha\n",
    "- different stemmers\n",
    "- N-gram range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.01,\n",
       " 0.016681005372000592,\n",
       " 0.027825594022071243,\n",
       " 0.046415888336127774,\n",
       " 0.077426368268112694,\n",
       " 0.12915496650148839,\n",
       " 0.21544346900318834,\n",
       " 0.35938136638046259,\n",
       " 0.59948425031894093,\n",
       " 1.0]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in np.logspace(-2,0, num=10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_list = ['Text', 'Stemmed', 'Porter', 'Lancaster']\n",
    "\n",
    "vectorizer_list = [\n",
    "        ('tfidf-char', TfidfVectorizer(analyzer='char', norm='l2', smooth_idf=False), {\n",
    "        \n",
    "        'tfidf-char__max_features': (None, 2000, 5000, 10000, 20000),\n",
    "        'tfidf-char__ngram_range': ((3, 4), (3, 5))})\n",
    "]\n",
    "classifier_list = [\n",
    "    \n",
    "    ('mnb', MultinomialNB(), {'mnb__alpha': (0.01,0.016,0.027,0.046,0.077,0.129,0.215,0.359,0.599,1.0)}),\n",
    "    \n",
    "    #('ada', AdaBoostClassifier(), {'ada__n_estimators': (10, 50, 100, 200)})\n",
    "]\n",
    "\n",
    "#run grid search on above parameters\n",
    "mnb_results = GridSearchCV_results( \n",
    "    vectorizer_list, classifier_list, X_train_list, X_train, y_train, score = 'accuracy' \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf-char Lancaster: 0.991792767376\n",
      "{'tfidf-char__max_features': 10000, 'tfidf-char__ngram_range': (3, 4), 'mnb__alpha': 0.129}\n",
      "tfidf-char Porter: 0.991792767376\n",
      "{'tfidf-char__max_features': 10000, 'tfidf-char__ngram_range': (3, 4), 'mnb__alpha': 0.129}\n",
      "tfidf-char Stemmed: 0.991792767376\n",
      "{'tfidf-char__max_features': 10000, 'tfidf-char__ngram_range': (3, 4), 'mnb__alpha': 0.129}\n",
      "tfidf-char Text: 0.991792767376\n",
      "{'tfidf-char__max_features': 10000, 'tfidf-char__ngram_range': (3, 4), 'mnb__alpha': 0.129}\n"
     ]
    }
   ],
   "source": [
    "for key, val in sorted(mnb_results.items()): \n",
    "    print(key[1] +  ' ' + key[0] + ': ' + str(val.best_score_))\n",
    "    print(val.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the best alpha in this group (.129) gives 99.18% accuracy, a .13% increase.  The results are the same for all of the stemmers, so let's just use the original text for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's see whethet AdaBoost gives any further improvement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_list = ['Text']\n",
    "\n",
    "vectorizer_list = [\n",
    "        ('tfidf-char', TfidfVectorizer(analyzer='char'), {\n",
    "        'tfidf-char__max_features': (10000,),\n",
    "        'tfidf-char__ngram_range': ((3, 4),),\n",
    "        'tfidf-char__norm': ('l2',)})\n",
    "]\n",
    "classifier_list = [\n",
    "    \n",
    "    ('mnb', MultinomialNB(), {'mnb__alpha': (0.129,)})\n",
    "    \n",
    "    ,('ada', AdaBoostClassifier(MultinomialNB(alpha=0.129)), \n",
    "        {'ada__n_estimators': (10, 50, 100, 200)})\n",
    "]\n",
    "adaboost_results = GridSearchCV_results(\n",
    "    vectorizer_list, classifier_list, X_train_list, X_train, y_train, score = 'accuracy' \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ada 0.980764298538\n",
      "{'ada__n_estimators': 100, 'tfidf-char__max_features': 10000, 'tfidf-char__norm': 'l2', 'tfidf-char__ngram_range': (3, 4)}\n",
      "mnb 0.991023339318\n",
      "{'tfidf-char__max_features': 10000, 'tfidf-char__norm': 'l2', 'tfidf-char__ngram_range': (3, 4), 'mnb__alpha': 0.129}\n"
     ]
    }
   ],
   "source": [
    "for key, val in adaboost_results.items(): \n",
    "    print(key[2], str(val.best_score_))\n",
    "    print(val.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So AdaBoost isn't better.  Looks like the Naive Bayes classifier give the best results on the training set, with an accuracy of 99.1%!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Run optimized classifier on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.9931\n",
      "Test set accuracy: 0.9844\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([('tfidf-char', TfidfVectorizer(analyzer='char', max_features=10000, ngram_range=(3, 4), norm='l2')) \\\n",
    "                            ,('mnb', MultinomialNB(alpha=0.129)) ])\n",
    "pipeline.fit(X_train['Text'],y_train)\n",
    "predictions = pipeline.predict(X_test['Text'])\n",
    "print('Training set accuracy: %.4f' % pipeline.score(X_train['Text'], y_train))\n",
    "print('Test set accuracy: %.4f' % pipeline.score(X_test['Text'], y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1418,    6],\n",
       "       [  20,  228]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for fun, let's see which items were misclassified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual status</th>\n",
       "      <th>Text of misclassified spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>1</td>\n",
       "      <td>Do you realize that in about 40 years, we'll have thousands of old ladies running around with tattoos?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0</td>\n",
       "      <td>Yup next stop.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5045</th>\n",
       "      <td>0</td>\n",
       "      <td>We have sent JD for Customer Service cum Accounts Executive to ur mail id, For details contact us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4255</th>\n",
       "      <td>1</td>\n",
       "      <td>Block Breaker now comes in deluxe format with new features and great graphics from T-Mobile. Buy for just £5 by replying GET BBDELUXE and take the challenge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3889</th>\n",
       "      <td>0</td>\n",
       "      <td>Unlimited texts. Limited minutes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1429</th>\n",
       "      <td>1</td>\n",
       "      <td>For sale - arsenal dartboard. Good condition but no doubles or trebles!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4297</th>\n",
       "      <td>1</td>\n",
       "      <td>thesmszone.com lets you send free anonymous and masked messages..im sending this message from there..do you see the potential for abuse???</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3573</th>\n",
       "      <td>1</td>\n",
       "      <td>You won't believe it but it's true. It's Incredible Txts! Reply G now to learn truly amazing things that will blow your mind. From O2FWD only 18p/txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3863</th>\n",
       "      <td>1</td>\n",
       "      <td>Oh my god! I've found your number again! I'm so glad, text me back xafter this msgs cst std ntwk chg £1.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3301</th>\n",
       "      <td>1</td>\n",
       "      <td>RCT' THNQ Adrian for U text. Rgds Vatian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>0</td>\n",
       "      <td>We have sent JD for Customer Service cum Accounts Executive to ur mail id, For details contact us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2246</th>\n",
       "      <td>1</td>\n",
       "      <td>Hi ya babe x u 4goten bout me?' scammers getting smart..Though this is a regular vodafone no, if you respond you get further prem rate msg/subscription. Other nos used also. Be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>1</td>\n",
       "      <td>SMS. ac sun0819 posts HELLO:\"You seem cool, wanted to say hi. HI!!!\" Stop? Send STOP to 62468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>1</td>\n",
       "      <td>Email AlertFrom: Jeri StewartSize: 2KBSubject: Low-cost prescripiton drvgsTo listen to email call 123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4068</th>\n",
       "      <td>1</td>\n",
       "      <td>TBS/PERSOLVO. been chasing us since Sept for£38 definitely not paying now thanks to your information. We will ignore them. Kath. Manchester.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3418</th>\n",
       "      <td>1</td>\n",
       "      <td>LIFE has never been this much fun and great until you came in. You made it truly special for me. I won't forget you! enjoy @ one gbp/sms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5009</th>\n",
       "      <td>0</td>\n",
       "      <td>My mobile number.pls sms ur mail id.convey regards to achan,amma.Rakhesh.Qatar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4513</th>\n",
       "      <td>1</td>\n",
       "      <td>Money i have won wining number 946 wot do i do next</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3859</th>\n",
       "      <td>1</td>\n",
       "      <td>Win the newest Harry Potter and the Order of the Phoenix (Book 5) reply HARRY, answer 5 questions - chance to be the first among readers!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>1</td>\n",
       "      <td>Talk sexy!! Make new friends or fall in love in the worlds most discreet text dating service. Just text VIP to 83110 and see who you could meet.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2820</th>\n",
       "      <td>1</td>\n",
       "      <td>INTERFLORA - It's not too late to order Interflora flowers for christmas call 0800 505060 to place your order before Midnight tomorrow.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>1</td>\n",
       "      <td>SMS. ac JSco: Energy is high, but u may not know where 2channel it. 2day ur leadership skills r strong. Psychic? Reply ANS w/question. End? Reply END JSCO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4948</th>\n",
       "      <td>1</td>\n",
       "      <td>Hi this is Amy, we will be sending you a free phone number in a couple of days, which will give you an access to all the adult parties...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>1</td>\n",
       "      <td>Filthy stories and GIRLS waiting for your</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>1</td>\n",
       "      <td>Hello. We need some posh birds and chaps to user trial prods for champneys. Can i put you down? I need your address and dob asap. Ta r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4677</th>\n",
       "      <td>0</td>\n",
       "      <td>Wewa is 130. Iriver 255. All 128 mb.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Actual status  \\\n",
       "750               1   \n",
       "83                0   \n",
       "5045              0   \n",
       "4255              1   \n",
       "3889              0   \n",
       "1429              1   \n",
       "4297              1   \n",
       "3573              1   \n",
       "3863              1   \n",
       "3301              1   \n",
       "1259              0   \n",
       "2246              1   \n",
       "671               1   \n",
       "730               1   \n",
       "4068              1   \n",
       "3418              1   \n",
       "5009              0   \n",
       "4513              1   \n",
       "3859              1   \n",
       "855               1   \n",
       "2820              1   \n",
       "1499              1   \n",
       "4948              1   \n",
       "954               1   \n",
       "868               1   \n",
       "4677              0   \n",
       "\n",
       "                                                                                                                                                               Text of misclassified spam  \n",
       "750                                                                                Do you realize that in about 40 years, we'll have thousands of old ladies running around with tattoos?  \n",
       "83                                                                                                                                                                         Yup next stop.  \n",
       "5045                                                                                    We have sent JD for Customer Service cum Accounts Executive to ur mail id, For details contact us  \n",
       "4255                         Block Breaker now comes in deluxe format with new features and great graphics from T-Mobile. Buy for just £5 by replying GET BBDELUXE and take the challenge  \n",
       "3889                                                                                                                                                    Unlimited texts. Limited minutes.  \n",
       "1429                                                                                                              For sale - arsenal dartboard. Good condition but no doubles or trebles!  \n",
       "4297                                           thesmszone.com lets you send free anonymous and masked messages..im sending this message from there..do you see the potential for abuse???  \n",
       "3573                                You won't believe it but it's true. It's Incredible Txts! Reply G now to learn truly amazing things that will blow your mind. From O2FWD only 18p/txt  \n",
       "3863                                                                           Oh my god! I've found your number again! I'm so glad, text me back xafter this msgs cst std ntwk chg £1.50  \n",
       "3301                                                                                                                                             RCT' THNQ Adrian for U text. Rgds Vatian  \n",
       "1259                                                                                    We have sent JD for Customer Service cum Accounts Executive to ur mail id, For details contact us  \n",
       "2246  Hi ya babe x u 4goten bout me?' scammers getting smart..Though this is a regular vodafone no, if you respond you get further prem rate msg/subscription. Other nos used also. Be...  \n",
       "671                                                                                         SMS. ac sun0819 posts HELLO:\"You seem cool, wanted to say hi. HI!!!\" Stop? Send STOP to 62468  \n",
       "730                                                                                 Email AlertFrom: Jeri StewartSize: 2KBSubject: Low-cost prescripiton drvgsTo listen to email call 123  \n",
       "4068                                         TBS/PERSOLVO. been chasing us since Sept for£38 definitely not paying now thanks to your information. We will ignore them. Kath. Manchester.  \n",
       "3418                                             LIFE has never been this much fun and great until you came in. You made it truly special for me. I won't forget you! enjoy @ one gbp/sms  \n",
       "5009                                                                                                       My mobile number.pls sms ur mail id.convey regards to achan,amma.Rakhesh.Qatar  \n",
       "4513                                                                                                                                  Money i have won wining number 946 wot do i do next  \n",
       "3859                                           Win the newest Harry Potter and the Order of the Phoenix (Book 5) reply HARRY, answer 5 questions - chance to be the first among readers!  \n",
       "855                                      Talk sexy!! Make new friends or fall in love in the worlds most discreet text dating service. Just text VIP to 83110 and see who you could meet.  \n",
       "2820                                             INTERFLORA - It's not too late to order Interflora flowers for christmas call 0800 505060 to place your order before Midnight tomorrow.  \n",
       "1499                           SMS. ac JSco: Energy is high, but u may not know where 2channel it. 2day ur leadership skills r strong. Psychic? Reply ANS w/question. End? Reply END JSCO  \n",
       "4948                                            Hi this is Amy, we will be sending you a free phone number in a couple of days, which will give you an access to all the adult parties...  \n",
       "954                                                                                                                                             Filthy stories and GIRLS waiting for your  \n",
       "868                                                Hello. We need some posh birds and chaps to user trial prods for champneys. Can i put you down? I need your address and dob asap. Ta r  \n",
       "4677                                                                                                                                                 Wewa is 130. Iriver 255. All 128 mb.  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 180\n",
    "pd.DataFrame(data = {'Text of misclassified spam':X_test['Text'][predictions != y_test], 'Actual status':y_test[predictions != y_test]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We were able to correctly classify about 98.4% of the test set by optimizing the parameters for a Naive Bayes Classifier.  This is slightly larger than the accuracy reported by Lantz (\"almost 98%\").\n",
    "\n",
    "To arrive at the optimal classifier, we explored:\n",
    "\n",
    "Different classification algorithms:\n",
    "- Naive Bayes\n",
    "- Linear Regression\n",
    "- Support Vector Machine (with linear, polynomial, and RBF kernels)\n",
    "- AdaBoost (on Naive Bayes)\n",
    "\n",
    "Stemming algorithms:\n",
    "- Original text\n",
    "- Porter\n",
    "- Snowball\n",
    "- Lancaster\n",
    "\n",
    "Vectorizers:\n",
    "- Count (at both the char and word level)\n",
    "- TF-IDF (at both the char and word level)\n",
    "\n",
    "Regularization parameters:\n",
    "- l2 vs l1 vs elastic\n",
    "- alpha\n",
    "\n",
    "The optimal parameter set was found to be:\n",
    "- Naive Bayes, with alpha = 0.129\n",
    "- Original text (no stemming)\n",
    "- Count vectorizer (with 3- and 4-char n-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
